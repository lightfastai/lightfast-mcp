# AI SDK Integration Approaches

This document compares different approaches for integrating AI models with the Blender MCP server.

## ğŸš« **Previous Approach (NOT Recommended)**

**Raw HTTP Calls with httpx.AsyncClient**
```python
# âŒ DON'T DO THIS
async with httpx.AsyncClient() as client:
    response = await client.post(
        "https://api.anthropic.com/v1/messages",
        headers={"Authorization": f"Bearer {api_key}"},
        json={"model": "claude-3-5-sonnet-20241022", "messages": [...]}
    )
```

**Problems:**
- âŒ Manual error handling
- âŒ No automatic retries
- âŒ Missing rate limiting
- âŒ No streaming support
- âŒ Authentication complexity
- âŒ Response parsing overhead

## âœ… **Recommended Approaches**

### **1. Official SDKs (BEST)**

**Anthropic SDK:**
```python
import anthropic

client = anthropic.AsyncAnthropic(api_key=api_key)
response = await client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=4000,
    messages=[{"role": "user", "content": message}]
)
```

**OpenAI SDK:**
```python
import openai

client = openai.AsyncOpenAI(api_key=api_key)
response = await client.chat.completions.create(
    model="gpt-4o",
    messages=[{"role": "user", "content": message}],
    max_tokens=4000
)
```

**Benefits:**
- âœ… Robust error handling and retries
- âœ… Automatic rate limiting
- âœ… Streaming support
- âœ… Type safety with proper models
- âœ… Regular updates and maintenance
- âœ… Built-in authentication handling

### **2. LiteLLM (Multi-Provider)**

**Unified Interface for Multiple Providers:**
```python
import litellm

# Works with 100+ providers using same interface
response = await litellm.acompletion(
    model="claude-3-5-sonnet-20241022",  # or "gpt-4o", "gemini-pro", etc.
    messages=[{"role": "user", "content": message}],
    max_tokens=4000
)
```

**Benefits:**
- âœ… Single interface for all providers
- âœ… Easy provider switching
- âœ… Consistent error handling
- âœ… Cost tracking and logging
- âœ… Fallback provider support

### **3. LangChain (Full Framework)**

**For Complex AI Applications:**
```python
from langchain_anthropic import ChatAnthropic
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

llm = ChatAnthropic(model_name="claude-3-5-sonnet-20241022")
response = await llm.ainvoke([HumanMessage(content=message)])
```

**Benefits:**
- âœ… Rich ecosystem of tools
- âœ… Built-in memory and agents
- âœ… Vector store integrations
- âœ… Complex workflow orchestration

## ğŸ“Š **Comparison Matrix**

| Feature | Raw HTTP | Official SDKs | LiteLLM | LangChain |
|---------|----------|---------------|---------|-----------|
| **Simplicity** | âŒ Low | âœ… High | âœ… High | âš ï¸ Medium |
| **Error Handling** | âŒ Manual | âœ… Built-in | âœ… Built-in | âœ… Built-in |
| **Multi-Provider** | âŒ No | âŒ No | âœ… Yes | âœ… Yes |
| **Type Safety** | âŒ No | âœ… Yes | âš ï¸ Partial | âœ… Yes |
| **Maintenance** | âŒ You | âœ… Official | âœ… Community | âœ… Community |
| **Learning Curve** | âš ï¸ Medium | âœ… Low | âœ… Low | âŒ High |
| **Performance** | âš ï¸ Manual | âœ… Optimized | âœ… Optimized | âš ï¸ Overhead |

## ğŸš€ **Implementation Examples**

### **Setup Dependencies**

```bash
# Install all approaches
uv add anthropic openai litellm langchain langchain-anthropic langchain-openai
```

### **Environment Variables**

```bash
# .env file
ANTHROPIC_API_KEY=sk-ant-your-key-here
OPENAI_API_KEY=sk-your-openai-key-here
AI_PROVIDER=claude  # or openai
AI_APPROACH=official  # official, litellm, langchain
```

### **Usage in Blender MCP Client**

```python
# Test different approaches
./scripts/test_blender.sh compare        # Compare all approaches
./scripts/test_blender.sh ai-improved    # Use improved client

# Environment variables
AI_APPROACH=official ./scripts/test_blender.sh ai-improved
AI_APPROACH=litellm ./scripts/test_blender.sh ai-improved
AI_PROVIDER=openai ./scripts/test_blender.sh ai-improved
```

## ğŸ¯ **Recommendations**

### **For Production Applications:**
1. **Use Official SDKs** (`anthropic`, `openai`) for single-provider applications
2. **Use LiteLLM** for multi-provider flexibility
3. **Use LangChain** only if you need complex AI workflows

### **For Development/Prototyping:**
1. Start with **Official SDKs** for simplicity
2. Switch to **LiteLLM** when you need provider flexibility
3. Avoid raw HTTP calls unless absolutely necessary

### **For Our Blender MCP Integration:**
- **Current Implementation**: Official SDKs (anthropic, openai)
- **Future Enhancement**: LiteLLM for provider flexibility
- **Advanced Features**: Consider LangChain for AI agents that can perform complex Blender tasks

## ğŸ”§ **Migration Guide**

### **From Raw HTTP to Official SDK:**

**Before (âŒ):**
```python
async with httpx.AsyncClient() as client:
    response = await client.post(
        "https://api.anthropic.com/v1/messages",
        headers={"Authorization": f"Bearer {api_key}"},
        json={"model": "claude-3-5-sonnet-20241022", "messages": messages}
    )
    result = response.json()
```

**After (âœ…):**
```python
client = anthropic.AsyncAnthropic(api_key=api_key)
response = await client.messages.create(
    model="claude-3-5-sonnet-20241022",
    messages=messages,
    max_tokens=4000
)
result = response.content[0].text
```

### **Benefits Gained:**
- ğŸš€ **60% less code**
- ğŸ›¡ï¸ **Built-in error handling**
- âš¡ **Automatic retries**
- ğŸ”„ **Connection pooling**
- ğŸ“Š **Better debugging** 